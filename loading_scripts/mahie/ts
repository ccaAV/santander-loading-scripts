#!/usr/bin/env python3
import re
import argparse
from statistics import mean
from collections import defaultdict

# ---------------- REGEX ----------------
ANSI_ESCAPE = re.compile(r"\x1B\[[0-?]*[ -/]*[@-~]")

START_PATTERN = re.compile(
    r"Transaction Started\s+transaction_id=(\d+).*?on_stores=\[(.*?)\]"
)

COMMIT_PATTERN = re.compile(
    r"Transaction Committed\s+transaction_id=(\d+).*?"
    r"transaction_duration=(\d+)ms\s+commit_duration=(\d+)ms"
)
# --------------------------------------


def strip_ansi(text: str) -> str:
    return ANSI_ESCAPE.sub("", text)


def parse_logs(log_file):
    transactions = defaultdict(dict)

    with open(log_file, "r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            line = strip_ansi(line)

            start_match = START_PATTERN.search(line)
            if start_match:
                tx_id = start_match.group(1)
                stores = {
                    s.strip() for s in start_match.group(2).split(",")
                }
                transactions[tx_id]["stores"] = stores
                continue

            commit_match = COMMIT_PATTERN.search(line)
            if commit_match:
                tx_id = commit_match.group(1)
                transactions[tx_id]["transaction_duration"] = int(commit_match.group(2))
                transactions[tx_id]["commit_duration"] = int(commit_match.group(3))

    return transactions


def filter_transactions(transactions, input_stores):
    filtered = []
    for tx_id, data in transactions.items():
        if (
            "stores" in data
            and "transaction_duration" in data
            and "commit_duration" in data
            and data["stores"] & input_stores
        ):
            filtered.append((tx_id, data))
    return filtered


def print_stats(values, label):
    print(f"\n{label}")
    print("-" * len(label))
    print(f"Count   : {len(values)}")
    print(f"Min     : {min(values)} ms")
    print(f"Max     : {max(values)} ms")
    print(f"Average : {int(mean(values))} ms")


def print_top_k(transactions, key, k):
    print(f"\nTop {k} transactions by {key}")
    print("-" * (28 + len(key)))
    top = sorted(transactions, key=lambda x: x[1][key], reverse=True)[:k]
    for tx_id, data in top:
        print(
            f"tx_id={tx_id} {key}={data[key]}ms "
            f"stores={sorted(data['stores'])}"
        )


def main():
    parser = argparse.ArgumentParser(
        description="Analyze datastore transactions from ActivePivot logs",
        formatter_class=argparse.RawTextHelpFormatter,
        epilog="""
Examples:
  python tx_stats.py application.log TradePnLs TradeSensitivities
  python tx_stats.py app.log TradePnLs --topk 10
"""
    )

    parser.add_argument(
        "logfile",
        help="Path to the log file to analyze"
    )

    parser.add_argument(
        "stores",
        nargs="+",
        help="Store names to match (transaction must contain at least one)"
    )

    parser.add_argument(
        "--topk",
        type=int,
        default=12,
        help="Number of slowest transactions to display (default: 5)"
    )

    args = parser.parse_args()

    input_stores = set(args.stores)

    transactions = parse_logs(args.logfile)
    filtered = filter_transactions(transactions, input_stores)

    if not filtered:
        print("No matching transactions found.")
        return

    tx_durations = [d["transaction_duration"] for _, d in filtered]
    commit_durations = [d["commit_duration"] for _, d in filtered]

    print_stats(tx_durations, "Transaction Duration Stats")
    print_stats(commit_durations, "Commit Duration Stats")

    print_top_k(filtered, "transaction_duration", args.topk)
    print_top_k(filtered, "commit_duration", args.topk)


if __name__ == "__main__":
    main()

